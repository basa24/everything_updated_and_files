# -*- coding: utf-8 -*-
"""FRCNN_inference_evaluation_each_class

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14Mds2LJsIS7LTi81bZpLXGbxp2gruW7U
"""

!pip install torch torchvision
import pandas as pd
import torch
import torchvision
from torchvision import transforms as T
import time
from PIL import Image
import cv2
import os
import json
from google.colab.patches import cv2_imshow
resolution = 640

coco_names = ["person" , "bicycle" , "car" , "motorcycle" , "airplane" , "bus" , "train" , "truck" , "boat" , "traffic light" , "fire hydrant" , "street sign" , "stop sign" , "parking meter" , "bench" , "bird" , "cat" , "dog" , "horse" , "sheep" , "cow" , "elephant" , "bear" , "zebra" , "giraffe" , "hat" , "backpack" , "umbrella" , "shoe" , "eye glasses" , "handbag" , "tie" , "suitcase" ,
"frisbee" , "skis" , "snowboard" , "sports ball" , "kite" , "baseball bat" ,
"baseball glove" , "skateboard" , "surfboard" , "tennis racket" , "bottle" ,
"plate" , "wine glass" , "cup" , "fork" , "knife" , "spoon" , "bowl" ,
"banana" , "apple" , "sandwich" , "orange" , "broccoli" , "carrot" , "hot dog" ,
"pizza" , "donut" , "cake" , "chair" , "couch" , "potted plant" , "bed" ,
"mirror" , "dining table" , "window" , "desk" , "toilet" , "door" , "tv" ,
"laptop" , "mouse" , "remote" , "keyboard" , "cell phone" , "microwave" ,
"oven" , "toaster" , "sink" , "refrigerator" , "blender" , "book" ,
"clock" , "vase" , "scissors" , "teddy bear" , "hair drier" , "toothbrush" , "hair brush"]

import torchvision.models.detection as detection

# Load the pre-trained Faster R-CNN model
model = detection.fasterrcnn_resnet50_fpn(pretrained=True)
if torch.cuda.is_available():
    model.cuda()  # Move model to GPU
if torch.cuda.is_available():
    print("GPU is available!")
else:
    print("GPU is not available.")

model.eval()

# Replace 'your_file.json' with the path to your actual JSON file

#dont delete this cell
file_path = '/content/drive/MyDrive/files and others for ee/imagesvalidationids.json'

with open(file_path, 'r') as file:
    data = json.load(file)
file_name_to_id = {item['file_name']: item['id'] for item in data}

print(file_name_to_id)

file_path = '/content/drive/MyDrive/files and others for ee/categoriesvalidation.json'
with open(file_path, 'r') as file:
    data = json.load(file)
categoryid_to_name = {item['id']: item['name'] for item in data}

print(categoryid_to_name)

#changing category id into class names
"""file_path = '/content/drive/MyDrive/files and others for ee/annotationsvalidation.json'
with open(file_path, 'r') as file:
    data = json.load(file)
for item in data:
    # Directly access 'id' and use it to update 'id' based on 'categoryid_to_name' mapping
    if item['category_id'] in categoryid_to_name:  # Check if the current 'id' exists in the mapping
        item['category_id'] = categoryid_to_name[item['category_id']]
new_file_path = 'modified_data.json'

with open(new_file_path, 'w') as file:
    json.dump(data, file, indent=4)
from google.colab import files

files.download('modified_data.json')"""

#chnaing bboxes format into x1,y1,x2,y2
"""def convert_bbox_format(bbox):
    x_min, y_min, width, height = bbox
    x_max = x_min + width
    y_max = y_min + height
    return [x_min, y_min, x_max, y_max]
file_path = '/content/drive/MyDrive/files and others for ee/modified_data.json'
with open(file_path, 'r') as file:
    data = json.load(file)
for item in data:
    boundingbox= item['bbox']
    item['bbox'] = convert_bbox_format(boundingbox)
new_file_path = 'modified_data_with_boundingboxes_try2.json'

with open(new_file_path, 'w') as file:
    json.dump(data, file, indent=4)
from google.colab import files

files.download('modified_data_with_boundingboxes_try2.json')"""

"""for key in name_to_categoryid.keys():
  word_to_check = key

# Use list comprehension to check if the word is in the array
  is_present = any(word_to_check == word for word in coco_names)

  if is_present:
    continue
  else:
    print(f"{word_to_check} is not present in the array.")"""

#Accessing validation images from mydrive

obj=[]
times=[]
scrs=[]
boxes=[]
image_names = []
image_id_perdetection=[]
from os import listdir
directory="/content/drive/MyDrive/valid"
for image in os.listdir(directory):
  image_path = "/content/drive/MyDrive/valid/"+image
  #print(image_path) works, every path accessed
  ig = Image.open(image_path)


  # Convert the resized image to tensor
  transform = T.ToTensor()
  img = transform(ig)


  start_time = time.time()

  with torch.no_grad():
    pred = model([img])

  end_time_model = time.time()  # Time after model prediction

  # Apply confidence threshold
  bboxes, labels, scores = pred[0]["boxes"], pred[0]["labels"], pred[0]["scores"]
  keep = scores > 0.5
  bboxes = bboxes[keep]
  labels = labels[keep]
  scores = scores[keep]

  end_time_filtering = time.time()  # Time after filtering

  elapsed_time_model = end_time_model - start_time
  elapsed_time_filtering = end_time_filtering - end_time_model
  elapsed_time_approx = elapsed_time_model + elapsed_time_filtering
  times.append(elapsed_time_approx)

  #print(f"Time taken for model inference: {elapsed_time_model:.3f} seconds")
  #print(f"Time taken for filtering: {elapsed_time_filtering:.3f} seconds")
  #print(f"Approximate inference time for high-confidence objects: {elapsed_time_approx:.3f} seconds")

  num = len(bboxes)  # Update number of detections

  # ... (rest of the code for drawing bounding boxes, etc.)
  #loop for each pred, all preds should be in an array

  image_id = file_name_to_id[image]
  for i in range(num):

    class_name = coco_names[labels.numpy()[i] - 1]

    scores_text = f"{class_name} {scores[i].item():.2f}"  # Corrected here
    int_bbox = bboxes[i].numpy().astype(int)
    boxes.append(int_bbox)
    obj.append(class_name)
    scrs.append(scores[i].item())
    image_id_perdetection.append(image_id)
    #comment this out if you plan to display the images by uncommentingb the cell below

#!wget http://images.cocodataset.org/val2017/000000037777.jpg



#pred #a list of length one of dictionary. keys are boxes, labels and scores).
#will see labels and scores(confidence).
#Top row shows bounding boxes.(x1y1 and x2y2)

#bboxes, labels, scores = pred[0]["boxes"], pred[0]["labels"], pred[0]["scores"]#storing in variables

#num = torch.argwhere(scores>0.5).shape[0]



#commented this out so that i loop without displaying boxes on the image
"""igg = cv2.imread(image_path)


obj=[]
scrs=[]

for i in range(num):  # Loop through each detection
    # Extracting the bounding box coordinates from the prediction
    x1, y1, x2, y2 = bboxes[i].numpy().astype("int")

    # Extracting the class name using the label from the prediction
    class_name = coco_names[labels.numpy()[i] - 1]

    # Drawing the bounding box on the image
    igg = cv2.rectangle(igg, (x1, y1), (x2, y2), (300, 200, 0), 1)

    # Formatting the scores text with the class name and confidence score
    # Ensure to extract individual score for each bounding box
    scores_text = f"{class_name} {scores[i].item():.2f}"  # Corrected here

    obj.append(class_name)
    scrs.append(scores[i].item())



    # Adding the text (class name and score) on top of the bounding box on the image
    igg = cv2.putText(igg, scores_text, (x1, y1 - 5), font, 0.5, (200, 0, 300), 1, cv2.LINE_AA)"""

#cv2_imshow(igg)

dict = {'image id': image_id_perdetection, 'name': obj, 'bounding box': boxes, 'confidence': scrs }

df = pd.DataFrame(dict)

pd.set_option('display.max_rows', None)  # This will allow all rows to be displayed
pd.set_option('display.max_columns', None)  # This will allow all columns to be displayed

df_sorted = df.sort_values(by='image id', ascending=True)


# If you want to modify the original DataFrame in-place, you can use:
# df.sort_values(by='image id', ascending=True, inplace=True)


#print("\n", elapsed_time_approx,"seconds") figure this out later
# Reset index to get the index as a column
df_sorted.reset_index(inplace=True, drop=True)

# Now add a new column 'bbox_id' which is just a sequence of numbers equal to the row number
df_sorted['bbox_id'] = df_sorted.index
df_sorted['bbox_id'] = df_sorted['bbox_id'].astype(int)

"""from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

# Initialize COCO ground truth and detection objects
coco_gt = COCO(ground_truth_json_path)  # Path to the COCO validation annotations
coco_dt = coco_gt.loadRes(predictions_json_path)  # Path to your model predictions in COCO format

# Initialize COCO Eval object
coco_eval = COCOeval(cocoGt=coco_gt, cocoDt=coco_dt, iouType='bbox')

# Evaluate
coco_eval.evaluate()
coco_eval.accumulate()
coco_eval.summarize()

# Access precision and recall from the summary

df_sorted.to_excel('detections.xlsx', index=False, engine='openpyxl')
from google.colab import files
files.download('detections.xlsx')#to save as an excel"
"""

#getting data frame from anotated images
file_path = '/content/drive/MyDrive/files and others for ee/modified_data_with_boundingboxes_try2.json'
with open(file_path, 'r') as file:
    data = json.load(file)

extracted_data = []
for item in data:
    extracted_data.append({
        'image id': item['image_id'],
        'name': item['category_id'],
        'bounding box': item['bbox']
    })

df_annot = pd.DataFrame(extracted_data)

df_annot.reset_index(inplace=True, drop=True)

# Now add a new column 'bbox_id' which is just a sequence of numbers equal to the row number
df_annot['gt_id'] = df_annot.index
df_annot['gt_id'] = df_annot['gt_id'].astype(int)

df_annot

#whats the main difference?
#why would we want to compare them
#what would be the optimal experimental set up
#whaat kind of data set would we input, the data set needs to be related to the pretraining, how did you get the data set
#ee proposal

# Assuming df_sorted and df_annot are your DataFrames

def calculate_iou(boxA, boxB):
    # Unpack the bounding box coordinates and calculate their areas
    xA, yA, wA, hA = boxA
    xB, yB, wB, hB = boxB
    areaA = wA * hA
    areaB = wB * hB

    # Convert to corner coordinates
    xAmin, yAmin, xAmax, yAmax = xA, yA, xA + wA, yA + hA
    xBmin, yBmin, xBmax, yBmax = xB, yB, xB + wB, yB + hB

    # Calculate intersection coordinates
    xImin = max(xAmin, xBmin)
    yImin = max(yAmin, yBmin)
    xImax = min(xAmax, xBmax)
    yImax = min(yAmax, yBmax)

    # Calculate intersection area
    interArea = max(xImax - xImin, 0) * max(yImax - yImin, 0)

    # Calculate IoU
    iou = interArea / float(areaA + areaB - interArea)
    return iou

# Sort df_sorted by 'confidence' to prioritize high confidence predictions
df_sorted = df_sorted.sort_values(by='confidence', ascending=False)

# Initialize trackers for matched ground truth and predictions
matched_gt = set()
matched_preds = set()

matches = []

for _, pred_row in df_sorted.iterrows():
    pred_box = pred_row['bounding box']
    bbox_id = pred_row['bbox_id']
    image_id = pred_row['image id']
    class_name = pred_row['name']
    confidence = pred_row['confidence']

    # Skip if this prediction has already been matched
    if bbox_id in matched_preds:
        continue

    # Filter ground truths for the same image ID and class name that haven't been matched
    gt_filtered = df_annot[(df_annot['image id'] == image_id) &
                           (df_annot['name'] == class_name) &
                           (~df_annot['gt_id'].isin(matched_gt))]

    best_iou = 0
    best_match = None

    for _, gt_row in gt_filtered.iterrows():
        gt_box = gt_row['bounding box']
        gt_id = gt_row['gt_id']
        iou = calculate_iou(pred_box, gt_box)

        if iou > best_iou:
            best_iou = iou
            best_match = (gt_row['gt_id'], gt_box)

    # If a suitable match is found (IoU > 0.5), record the match
    if best_match and best_iou > 0.5:
        matched_gt.add(best_match[0])  # Mark this gt_id as matched
        matched_preds.add(bbox_id)  # Mark this bbox_id as matched

        matches.append({
            'image id': image_id,
            'class name': class_name,
            'pred box': pred_box,
            'gt box': best_match[1],
            'iou': best_iou,
            'confidence': confidence,
            'bbox_id': bbox_id,
            'gt_id': best_match[0]
        })

# Convert matches to DataFrame for easier analysis
df_matches_with_confidence = pd.DataFrame(matches)

# Ensure df_matches_with_confidence now represents a strict one-to-one matching
# Check for one-to-one mapping of bbox_id to gt_id
# A one-to-one mapping means each bbox_id and gt_id appears exactly once

# Checking for any predicted bbox_id being mapped to more than one gt_id
pred_to_gt_mapping_issues = df_matches_with_confidence.duplicated(subset=['bbox_id'], keep=False)
if pred_to_gt_mapping_issues.any():
    print("Issues found with predicted bounding boxes being mapped to multiple ground truth boxes:")
    print(df_matches_with_confidence[pred_to_gt_mapping_issues].sort_values(by='bbox_id'))
else:
    print("All predicted bounding boxes are uniquely mapped to ground truth boxes.")

# Checking for any ground truth gt_id being mapped to more than one bbox_id
gt_to_pred_mapping_issues = df_matches_with_confidence.duplicated(subset=['gt_id'], keep=False)
if gt_to_pred_mapping_issues.any():
    print("\nIssues found with ground truth boxes being mapped to multiple predicted bounding boxes:")
    print(df_matches_with_confidence[gt_to_pred_mapping_issues].sort_values(by='gt_id'))
else:
    print("\nAll ground truth boxes are uniquely mapped to predicted bounding boxes.")

df_tp = df_matches_with_confidence[df_matches_with_confidence['iou'] >= 0.5]

unique_classes = df_annot['name'].unique()
class_metrics = {}

for class_name in unique_classes:
    # Filter true positives for current class
    tp_class = df_tp[df_tp['class name'] == class_name]

    # TP for current class
    tp = len(tp_class)

    # FP calculations
    total_preds_for_class = len(df_sorted[df_sorted['name'] == class_name])
    fp = total_preds_for_class - tp

    # FN calculations
    total_gts_for_class = len(df_annot[df_annot['name'] == class_name])
    fn = total_gts_for_class - tp

    # Calculate precision, recall, and F1 score
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    # Calculate average IoU for true positives of this class
    average_iou = tp_class['iou'].mean() if not tp_class.empty else 0

    # Storing the calculated metrics
    class_metrics[class_name] = {
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1_score,
        'TP': tp,
        'FP': fp,
        'FN': fn,
        'Average IoU': average_iou
    }

# Convert to DataFrame for easier viewing
df_class_metrics = pd.DataFrame.from_dict(class_metrics, orient='index')
df_class_metrics

# Define the filename
filename = "class_metrics_frcnn.xlsx"

# Save the DataFrame to an Excel file
df_class_metrics.to_excel(filename, index=True)

from google.colab import files

# Download the file to your local machine
files.download(filename)

unique_class_names = df_sorted['name'].unique().tolist()
count = 0
for i in unique_class_names:
  count = count +1

print(count)